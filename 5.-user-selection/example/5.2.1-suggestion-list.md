# 5.2.1 Suggestion List

## 5.2.1.1 Model-Specific

The following features are selected by the user:

* Interaction
* Parametric
* Algorithmic complexity

According to the score table, we can get:

|  | Interaction | Parametric | Algorithmic complexity | SUM | AVERAGE | SUGGESTION |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| GLM | 1 | 1 | 1 | 3 | 2 | STRONG |
| Gradient Boosting | 0.7 | 1 | 0.8 | 2.5 | 2 | STRONG |
| XGBoost | 0.7 | 0.9 | 0.7 | 2.3 | 2 | STRONG |
| Distributed Random Forest | 0.7 | 1 | 0.9 | 2.6 | 2 | STRONG |
| Deep Learning | 0 | 0.1 | 0.2 | 0.3 | 2 | LIGHTLY |
| K-means | 0.1 | 0.7 | 0.5 | 1.3 | 2 | MEDIUM |

Therefore, the Model-Specific methods we suggest using are:

* GLM
* Gradient Boosting
* XGBoost
* Distributed Random Forest

## 5.2.1.2 Model-Agnostic

The following features are selected by the user:

* Model internals
* Datapoint
* Detailed
* Certainty 

According to the score table, we can get:

|  | Model internals | Datapoint | Detailed | Certainty | SUM | AVERAGE | SUGGESTION |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Variable Importance / Standardized Coefficient | 1 | 0.5 | 1 | 1 | 3.5 | 2.6 | Strongly |
| PDP | 0.1 | 0.9 | 0.5 | 0.8 | 2.3 | 2.6 | Medium |
| ICE | 0.2 | 1 | 1 | 0.9 | 3.1 | 2.6 | Strongly |
| ALE | 0 | 0.9 | 0.6 | 0 | 1.5 | 2.6 | Lightly |

Therefore, the Model-Agnostic methods we suggest using are 

* Variable Importance
* PDP
* ICE.

